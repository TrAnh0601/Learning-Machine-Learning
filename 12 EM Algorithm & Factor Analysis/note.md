# The Expectation-Maximization (EM) Algorithm and Factor Analysis


## 1. Introduction to Latent Variable Models

In unsupervised learning, we are provided with an unlabeled dataset $\{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\}$. Our goal is to discover underlying structures, such as clusters or low-dimensional representations.

While algorithms like K-means provide hard assignments (each point belongs to exactly one cluster), probabilistic models like the Gaussian Mixture Model (GMM) provide soft assignments. GMMs assume the data is generated by multiple Gaussian distributions, but the exact source distribution for each data point is unobserved. These unobserved sources are called **latent variables**, denoted as $z^{(i)}$.

The joint distribution is given by:

$$p(x^{(i)}, z^{(i)}; \theta) = p(x^{(i)} \mid z^{(i)}; \theta) \, p(z^{(i)}; \theta)$$

If $z^{(i)}$ were known, parameter estimation would be a simple Maximum Likelihood Estimation (MLE). Because $z^{(i)}$ is hidden, maximizing the marginal log-likelihood requires specialized machinery: the EM algorithm.


## 2. Mathematical Foundations of EM

### 2.1 The Objective

We wish to maximize the log-likelihood of the parameters $\theta$:

$$\ell(\theta) = \sum_{i=1}^{m} \log p(x^{(i)}; \theta) = \sum_{i=1}^{m} \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta)$$

Direct differentiation is intractable because the summation $\sum_z$ lies inside the non-linear log function.

### 2.2 Jensen's Inequality

To bypass this, we utilize Jensen's Inequality. For a strictly concave function like $f(x) = \log(x)$, and a random variable $X$:

$$E[f(X)] \leq f(E[X])$$

Equality holds if and only if $X$ is a constant.

### 2.3 Constructing the Evidence Lower Bound (ELBO)

For each example $i$, we introduce an arbitrary probability distribution $Q_i(z^{(i)})$ over the latent variables, such that $\sum_z Q_i(z) = 1$. We rewrite the log-likelihood:

$$\ell(\theta) = \sum_{i=1}^{m} \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}$$

Recognizing the inner sum as an expectation $\mathbb{E}_{z \sim Q_i}$, we apply Jensen's Inequality:

$$\ell(\theta) \geq \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}$$

This right-hand side is the **Evidence Lower Bound (ELBO)**, denoted as $\mathcal{L}(\theta, Q)$.

### 2.4 ELBO as KL Divergence — The Variational View *(Research Extension)*

Rewriting the ELBO by separating terms reveals a critical structural identity:

$$\ell(\theta) = \mathcal{L}(\theta, Q) + KL\!\left(Q_i(z) \,\|\, p(z^{(i)} \mid x^{(i)}; \theta)\right)$$

where:

$$\mathcal{L}(\theta, Q) = \mathbb{E}_{z \sim Q}\!\left[\log p(x, z; \theta)\right] + \mathbb{H}[Q]$$

Expanding: $\mathcal{L} = \underbrace{\mathbb{E}_{z \sim Q}[\log p(x,z;\theta)]}_{\text{expected complete-data log-likelihood}} - \underbrace{\mathbb{E}_{z \sim Q}[\log Q(z)]}_{\text{negative entropy of } Q}$

**Implications:**

- Since $KL \geq 0$, we confirm $\ell(\theta) \geq \mathcal{L}(\theta, Q)$ — ELBO is a valid lower bound.
- The gap equals exactly the KL divergence between the variational distribution $Q$ and the true posterior $p(z \mid x; \theta)$.
- **E-step = minimizing this KL gap** by setting $Q_i = p(z^{(i)} \mid x^{(i)}; \theta)$, making the bound tight.
- This is the conceptual backbone of **Variational Inference** and **VAEs** — see Section 7.


## 3. The General EM Algorithm

The EM algorithm is an iterative procedure that maximizes $\ell(\theta)$ by alternating between maximizing the ELBO with respect to $Q$ and with respect to $\theta$.

### 3.1 The E-Step (Expectation)

Hold $\theta$ fixed and optimize $Q_i$. To make Jensen's inequality hold with equality, the term inside the log must be constant with respect to $z^{(i)}$:

$$\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} = C$$

Since $Q_i$ must sum to 1, normalizing yields the posterior:

$$Q_i(z^{(i)}) = p(z^{(i)} \mid x^{(i)}; \theta)$$

### 3.2 The M-Step (Maximization)

Hold $Q_i$ fixed and maximize the ELBO with respect to $\theta$:

$$\theta := \arg\max_{\theta} \sum_{i=1}^{m} \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}$$

Because $Q_i(z^{(i)})$ is treated as a constant weight, this reduces to a **weighted MLE** problem.

### 3.3 Convergence Guarantee

**Claim:** EM monotonically increases the marginal log-likelihood: $\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)})$.

**Proof sketch:**

Let $\theta^{(t)}$ be the parameters at iteration $t$. After the E-step, the bound is tight:

$$\ell(\theta^{(t)}) = \mathcal{L}(\theta^{(t)}, Q^{(t)}) \quad \text{(since KL gap} = 0\text{)}$$

The M-step finds $\theta^{(t+1)}$ that maximizes $\mathcal{L}$, so:

$$\mathcal{L}(\theta^{(t+1)}, Q^{(t)}) \geq \mathcal{L}(\theta^{(t)}, Q^{(t)}) = \ell(\theta^{(t)})$$

Since $\ell(\theta^{(t+1)}) \geq \mathcal{L}(\theta^{(t+1)}, Q^{(t)})$ always holds:

$$\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)}) \quad \blacksquare$$

**Critical caveats:**

- EM converges to a **local optimum**, not necessarily global.
- **Highly sensitive to initialization** — in practice, run multiple random restarts. For GMMs, K-means++ initialization is a common heuristic.
- Convergence can be extremely slow near saddle points (flat likelihood landscape).


## 4. Application 1: Gaussian Mixture Models (GMM)

Applying the general EM framework to GMMs yields the following specific update rules:

**E-Step** — Compute responsibilities (soft cluster assignments):

$$w_j^{(i)} := Q_i(z^{(i)} = j) = \frac{\phi_j \, \mathcal{N}(x^{(i)}; \mu_j, \Sigma_j)}{\sum_{k=1}^{K} \phi_k \, \mathcal{N}(x^{(i)}; \mu_k, \Sigma_k)}$$

**M-Step** — Update parameters using responsibilities as weights:

$$\mu_j := \frac{\sum_{i=1}^{m} w_j^{(i)} \, x^{(i)}}{\sum_{i=1}^{m} w_j^{(i)}}, \qquad \phi_j := \frac{1}{m} \sum_{i=1}^{m} w_j^{(i)}$$

$$\Sigma_j := \frac{\sum_{i=1}^{m} w_j^{(i)} (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^T}{\sum_{i=1}^{m} w_j^{(i)}}$$


## 5. Application 2: Factor Analysis

### 5.1 Motivation: High-Dimensional, Sparse Data

When the number of features is much greater than the number of training examples ($n \gg m$), computing a standard Gaussian covariance matrix $\Sigma$ results in a singular (non-invertible) matrix. This forces the Gaussian density to collapse, rendering it useless. Factor Analysis solves this by assuming the data fundamentally lies on a lower-dimensional subspace ($d < n$), with added independent noise.

### 5.2 The Factor Analysis Model

We define a continuous latent variable $z \in \mathbb{R}^d$ and an observed variable $x \in \mathbb{R}^n$:

1. $z \sim \mathcal{N}(0, I)$
2. $x = \mu + \Lambda z + \epsilon$
3. $\epsilon \sim \mathcal{N}(0, \Psi)$, where $\Psi$ is a strictly **diagonal** covariance matrix.

This implies:

$$x \mid z \sim \mathcal{N}(\mu + \Lambda z,\; \Psi)$$

The parameters to optimize are $\theta = \{\mu, \Lambda, \Psi\}$, where $\Lambda \in \mathbb{R}^{n \times d}$ is the **factor loading matrix**.

The marginal distribution of $x$ is:

$$x \sim \mathcal{N}(\mu,\; \Lambda\Lambda^T + \Psi)$$

This parameterizes a full-rank-like covariance $\Lambda\Lambda^T + \Psi$ using only $O(nd + n)$ parameters, versus $O(n^2)$ for an unconstrained $\Sigma$ — this is the core regularization.


## 6. Meta-Insights & Conclusion

**Coordinate Ascent Intuition:** The EM algorithm can be elegantly viewed as Coordinate Ascent on the surface of $\mathcal{L}(\theta, Q)$:
- The E-step optimizes $\mathcal{L}$ w.r.t. $Q$, making the bound tight (KL gap $= 0$).
- The M-step optimizes $\mathcal{L}$ w.r.t. $\theta$, pushing the lower bound (and consequently the true likelihood) upward.

**Generalization Power:** The beauty of the EM derivation is its universal applicability. Whether the latent variables are discrete cluster assignments (GMMs) or continuous low-dimensional representations (Factor Analysis), the fundamental mathematical mechanics — Jensen's Inequality and the ELBO — remain exactly the same.

**Small Data Paradigm:** Factor Analysis highlights a critical real-world engineering challenge. While deep learning excels in massive data regimes, Factor Analysis allows rigorous, robust density estimation and anomaly detection when data is extremely scarce but highly dimensional.
